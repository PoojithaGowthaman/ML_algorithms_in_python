{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISITC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* typically a binary classification problem - Class 1 or Class 0\n",
    "* Project the group scores onto a sigmoid function that better covers both the classes than a straight line \n",
    "* Set a threshold to the best and classify into groups\n",
    "\n",
    "\n",
    "### Mathematically\n",
    "* We have likelihood functions for x, y and beta.\n",
    "* Estimate the Beta - MLE. Select the beta value that maximizes the probability of observing data into the right class for the given vector of x points. For the given set of x values, a probability of y  in class 1 or class 0 is projected. This is the likelihood of Beta. Now we maximize this.\n",
    "* The maximizing is done in two steps - Take the log likelihood, apply gradient descent. To apply the GD, we use the log loss function, that is the exact opposite of Log likelihood. We start by randomly choosing the beta value, keep iterating and arrive at the minimum error WHICH becomes the MLE of Beta\n",
    "* Use learning rate to control the gradient of the Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main function\n",
    "def logistic_regression(x,y, iterations = 100, learning_rate = 0.01):\n",
    "    m,n = len(x), len(x[0])\n",
    "    beta_0, beta_other = initialize_params(n)\n",
    "    for i in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = (compute_gradients(x,y,beta_0, beta_other,m,n, 50))\n",
    "        beta_0, beta_other = update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate)\n",
    "        return beta_0, beta_other\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supporting functions\n",
    "# initializing the beta parameters,random start for the gradient descent\n",
    "def initialize_params(dimensions):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for i in range(dimensions)]\n",
    "    return beta_0, beta_other\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute functions\n",
    "\n",
    "def compute_gradients(x,y,beta_0, beta_other, n,m):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0]*n\n",
    "    \n",
    "    for i , point in enumerate(x): # computing gradients for each data point in x\n",
    "        prediction = logistic_regression(point, beta_0, beta_other) # getting the prediction for that point \n",
    "        \n",
    "        for j, features in enumerate(point): # compute the gradient for that single point\n",
    "            gradient_beta_other[j] += (pred - y[i])*features/m #similar to the function I wrote for Gradient at Betaj. Accumulate the data point from all data points and  normalize them by /m\n",
    "            gradient_beta_0 += (pred - y[i])/m\n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini - Batch gradient descent\n",
    "def compute_gradients_minibatch(x,y,beta_0, beta_other, n,m, batch_size):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0]*n\n",
    "    \n",
    "    for i in range(batch_size): # we pick a batch size and perform validation. Something like cross validatipn\n",
    "        i = random.randint(0, m-1)\n",
    "        point = x[i]\n",
    "        prediction = logistic_regression(point, beta_0, beta_other) # getting the prediction for that point \n",
    "        \n",
    "        for j, features in enumerate(point): # compute the gradient for that single point\n",
    "            gradient_beta_other[j] += (pred - y[i])*features/m #similar to the function I wrote for Gradient at Betaj. Accumulate the data point from all data points and  normalize them by /m\n",
    "            gradient_beta_0 += (pred - y[i])/m\n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the paramters\n",
    "\n",
    "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    beta_0 -= gradient_beta_0 * learning_rate\n",
    "    \n",
    "    for i in range(len(beta_other)):\n",
    "        beta_other[i] -= (gradient_beta_other[i])*learning_rate\n",
    "        return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
